#!/bin/bash

#SBATCH -A STF218
#SBATCH -J bigbird_frontier
#SBATCH -o logs/bigbird_oscar_frontier_10N-%j.o
#SBATCH -e logs/bigbird_oscar_frontier_10N-%j.e
#SBATCH -t 00:20:00
#SBATCH -p batch
#SBATCH -N 10

set +x
source /lustre/orion/proj-shared/stf006/irl1/conda/etc/profile.d/conda.sh
conda activate /lustre/orion/stf006/proj-shared/irl1/t2
export LD_PRELOAD="/usr/lib64/libcrypto.so /usr/lib64/libssh.so.4 /usr/lib64/libssl.so.1.1"

# setup hostfile
HOSTS=.hosts-job$SLURM_JOB_ID
HOSTFILE=hostfile.txt
srun hostname > $HOSTS
sed 's/$/ slots=8/' $HOSTS > $HOSTFILE

scontrol show hostnames $SLURM_NODELIST > job.node.list
input="./job.node.list"
readarray -t arr <"$input"
first=${arr[0]}
echo "first=" $first
ips=`ssh $first hostname -I`
read -ra arr <<< ${ips}
export MASTER_ADDR=${arr[0]}
echo "MASTER_ADDR=" $MASTER_ADDR

ranks_per_node=8
gpus_per_rank=$((8/$ranks_per_node))
ranks_total=$(($ranks_per_node*$SLURM_JOB_NUM_NODES))

mkdir logs
mkdir logs/transformer

export OMP_NUM_THREADS=2

srun -n $ranks_total -c 2 --ntasks_per-node=8 python /lustre/orion/stf006/proj-shared/irl1/nnUNet/nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task932_MBrain 0 -run_name find_lr --use_compressed_data --find_lr
