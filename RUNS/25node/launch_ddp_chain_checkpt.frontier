#!/bin/bash

#SBATCH -A BIF146
#SBATCH -J ddp
##SBATCH -o ddp-%j.o
##SBATCH -e ddp-%j.e
#SBATCH -t 02:00:00
#SBATCH -p batch
#SBATCH -N 25

set +x
source /lustre/orion/proj-shared/stf006/irl1/conda/etc/profile.d/conda.sh
conda activate /lustre/orion/bif146/world-shared/irl1/NNUNETENV

export nnUNet_raw_data_base=/lustre/orion/bif146/proj-shared/irl1/raw_data
#export nnUNet_preprocessed=/lustre/orion/bif146/proj-shared/irl1/preprocessed_data
export nnUNet_preprocessed=/lustre/orion/bif146/proj-shared/irl1/preprocessed_data/bigger
#export nnUNet_preprocessed=/lustre/orion/bif146/proj-shared/irl1/preprocessed_data/toobig
#export nnUNet_preprocessed=/lustre/orion/bif146/proj-shared/irl1/preprocessed_data/toobig2
export RESULTS_FOLDER=/lustre/orion/bif146/proj-shared/irl1/RESULTS

module load rocm/6.0.0

echo $SLURM_NODELIST


## setup hostfile
#HOSTS=.hosts-job$SLURM_JOB_ID
#HOSTFILE=hostfile.txt
#srun hostname > $HOSTS
#sed 's/$/ slots=8/' $HOSTS > $HOSTFILE
#
scontrol show hostnames $SLURM_NODELIST > job.node.list
#input="./job.node.list"
#readarray -t arr <"$input"
#first=${arr[0]}
#echo "first=" $first
#ips=`ssh $first hostname -I`
#read -ra arr <<< ${ips}
#MASTER_ADDR=${arr[0]}
#echo "MASTER_ADDR=" $MASTER_ADDR
#MASTER_ADDR2=${arr[2]}
#echo "MASTER_ADDR2=" $MASTER_ADDR2

folds=5
ranks_per_node=8
#gpus_per_rank=$((8/$ranks_per_node))
ranks_total=$(($ranks_per_node*$SLURM_JOB_NUM_NODES))
ranks_per_job=$(($ranks_per_node*$SLURM_JOB_NUM_NODES/$folds))
nodes_per_job=$(($SLURM_JOB_NUM_NODES/$folds))

declare -A nodelist_arr

for ((i=0;i<$folds;i++))
do
  st=$((nodes_per_job*i+1))
  for ((j=1;j<=$nodes_per_job;j++))
  do
    ij=$((nodes_per_job*i+j))
    if [ $ij -eq $st ]; then
      nodelist=$(cat job.node.list | head -n $ij | tail -1)
      #echo $nodelist
    else
      nodelist+=","
      #echo $i
      #cat oldjobnodelist | head -n $i | tail -1
      nodelist+=$(cat job.node.list | head -n $ij | tail -1)
    fi
  done
  nodelist_arr[$i]=$nodelist
done
for ((i=0;i<$folds;i++)); do
  echo "${nodelist_arr[$i]} "
done

export OMP_NUM_THREADS=1

#CPU Threading of unpacing npz files -- Not done, consider adding
#export nnUNet_def_n_proc=1
#Default value of data augmentation threads = 12, needs testing
#export nnUNet_n_proc_DA=4

#export NCCL_SOCKET_IFNAME=hsn
#export NCCL_PROTO=Simple
#export NCCL_DEBUG=info
#export FI_CXI_ATS=0
#export FI_LOG_LEVEL=info
export LD_LIBRARY_PATH=/lustre/orion/bif146/world-shared/irl1/aws-ofi-rccl-2/src/.libs/:${LD_LIBRARY_PATH}
export NCCL_NET_GDR_LEVEL=3
export FI_MR_CACHE_MONITOR=userfaultfd

# 7 cpus-per-task because 1 cpu kept for OS
#MIOPEN_DISABLE_CACHE=1 MIOPEN_CUSTOM_CACHE_DIR='pwd' HOME=/tmp/ srun -n $ranks_total -c 7 --ntasks-per-node=8 python nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task934_MBrainLat_dup 0 -run_name 8_node_scale_dup_dist_aws_singleThread --master_addr=$MASTER_ADDR
MIOPEN_DISABLE_CACHE=1 MIOPEN_CUSTOM_CACHE_DIR='pwd' HOME=/tmp/ srun -N $nodes_per_job -n $ranks_per_job -c 7 --nodelist ${nodelist_arr[0]} bash -c "source export_DDP_envvars_split.sh && python ../../nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task933_MBrainLat 0 -c -run_name chain_25n" &
MIOPEN_DISABLE_CACHE=1 MIOPEN_CUSTOM_CACHE_DIR='pwd' HOME=/tmp/ srun -N $nodes_per_job -n $ranks_per_job -c 7 --nodelist ${nodelist_arr[1]} bash -c "source export_DDP_envvars_split.sh && python ../../nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task933_MBrainLat 1 -c -run_name chain_25n" &
MIOPEN_DISABLE_CACHE=1 MIOPEN_CUSTOM_CACHE_DIR='pwd' HOME=/tmp/ srun -N $nodes_per_job -n $ranks_per_job -c 7 --nodelist ${nodelist_arr[2]} bash -c "source export_DDP_envvars_split.sh && python ../../nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task933_MBrainLat 2 -c -run_name chain_25n" &
MIOPEN_DISABLE_CACHE=1 MIOPEN_CUSTOM_CACHE_DIR='pwd' HOME=/tmp/ srun -N $nodes_per_job -n $ranks_per_job -c 7 --nodelist ${nodelist_arr[3]} bash -c "source export_DDP_envvars_split.sh && python ../../nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task933_MBrainLat 3 -c -run_name chain_25n" &
MIOPEN_DISABLE_CACHE=1 MIOPEN_CUSTOM_CACHE_DIR='pwd' HOME=/tmp/ srun -N $nodes_per_job -n $ranks_per_job -c 7 --nodelist ${nodelist_arr[4]} bash -c "source export_DDP_envvars_split.sh && python ../../nnunet/run/run_training_DDP.py 3d_fullres nnUNetTrainerV2_DDP Task933_MBrainLat 4 -c -run_name chain_25n" &
wait
